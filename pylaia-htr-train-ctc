#!/usr/bin/env python
from __future__ import absolute_import

import argparse
import os

import torch
from torch.optim import RMSprop

import laia.common.logging as log
from laia.common.arguments import add_argument, args, add_defaults
from laia.common.arguments_types import NumberInOpenRange
from laia.common.loader import ModelLoader, StateCheckpointLoader
from laia.common.random import manual_seed
from laia.common.saver import (
    CheckpointSaver,
    RollingSaver,
    ModelCheckpointSaver,
    StateCheckpointSaver,
)
from laia.conditions import Lowest, MultipleOf, GEqThan, ConsecutiveNonDecreasing
from laia.data import ImageDataLoader, TextImageFromTextTableDataset, FixedSizeSampler
from laia.engine import Trainer, Evaluator
from laia.engine.engine import EPOCH_END, EPOCH_START
from laia.engine.feeders import ImageFeeder, ItemFeeder
from laia.experiments.htr_experiment import HTRExperiment
from laia.hooks import Hook, HookCollection, action, Action
from laia.utils import SymbolsTable, ImageToTensor, TextToTensor

if __name__ == "__main__":
    add_defaults(
        "batch_size",
        "learning_rate",
        "momentum",
        "gpu",
        "max_epochs",
        "seed",
        "show_progress_bar",
        "train_path",
        "train_samples_per_epoch",
        "valid_samples_per_epoch",
        "iterations_per_update",
        "save_checkpoint_interval",
        "num_rolling_checkpoints",
    )
    add_argument(
        "syms",
        type=argparse.FileType("r"),
        help="Symbols table mapping from strings to integers",
    )
    add_argument(
        "img_dirs", type=str, nargs="+", help="Directory containing word images"
    )
    add_argument(
        "tr_txt_table",
        type=argparse.FileType("r"),
        help="Character transcriptions of each training image",
    )
    add_argument(
        "va_txt_table",
        type=argparse.FileType("r"),
        help="Character transcriptions of each validation image",
    )
    add_argument(
        "--delimiters",
        type=str,
        nargs="+",
        default=["<space>"],
        help="Sequence of characters representing the word delimiters",
    )
    add_argument(
        "--max_nondecreasing_epochs",
        type=NumberInOpenRange(int, vmin=0),
        help="Stop the training once there has been this number "
        "consecutive epochs without a new lowest validation CER",
    )
    add_argument(
        "--model_filename", type=str, default="model", help="File name of the model"
    )
    add_argument(
        "--checkpoint",
        type=str,
        default="ckpt.lowest-valid-cer*",
        help="Suffix of the checkpoint to use, can be a glob pattern",
    )
    args = args()

    manual_seed(args.seed)
    syms = SymbolsTable(args.syms)
    device = torch.device("cuda:{}".format(args.gpu - 1) if args.gpu else "cpu")

    model = ModelLoader(
        args.train_path, filename=args.model_filename, device=device
    ).load()
    if model is None:
        log.error('Could not find the model. Have you run "pylaia-htr-create-model"?')
        exit(1)
    model = model.to(device)

    tr_dataset = TextImageFromTextTableDataset(
        args.tr_txt_table,
        args.img_dirs,
        img_transform=ImageToTensor(),
        txt_transform=TextToTensor(syms),
    )
    tr_dataset_loader = ImageDataLoader(
        dataset=tr_dataset,
        image_channels=1,
        batch_size=args.batch_size,
        num_workers=8,
        shuffle=not bool(args.train_samples_per_epoch),
        sampler=FixedSizeSampler(tr_dataset, args.train_samples_per_epoch)
        if args.train_samples_per_epoch
        else None,
    )
    trainer = Trainer(
        model=model,
        criterion=None,  # Set automatically by HTRExperiment
        optimizer=RMSprop(
            model.parameters(), lr=args.learning_rate, momentum=args.momentum
        ),
        data_loader=tr_dataset_loader,
        batch_input_fn=ImageFeeder(device=device, parent_feeder=ItemFeeder("img")),
        batch_target_fn=ItemFeeder("txt"),
        batch_id_fn=ItemFeeder("id"),  # Print image ids on exception
        progress_bar="Train" if args.show_progress_bar else None,
        iterations_per_update=args.iterations_per_update,
    )

    va_dataset = TextImageFromTextTableDataset(
        args.va_txt_table,
        args.img_dirs,
        img_transform=ImageToTensor(),
        txt_transform=TextToTensor(syms),
    )
    va_dataset_loader = ImageDataLoader(
        dataset=va_dataset,
        image_channels=1,
        batch_size=args.batch_size,
        num_workers=8,
        sampler=FixedSizeSampler(va_dataset, args.valid_samples_per_epoch)
        if args.valid_samples_per_epoch
        else None,
    )
    evaluator = Evaluator(
        model=model,
        data_loader=va_dataset_loader,
        batch_input_fn=ImageFeeder(device=device, parent_feeder=ItemFeeder("img")),
        batch_target_fn=ItemFeeder("txt"),
        batch_id_fn=ItemFeeder("id"),
        progress_bar="Valid" if args.show_progress_bar else None,
    )

    experiment = HTRExperiment(
        trainer, evaluator, word_delimiters=[syms[sym] for sym in args.delimiters]
    )

    def ckpt_saver(filename, obj):
        return RollingSaver(
            StateCheckpointSaver(
                CheckpointSaver(os.path.join(args.train_path, filename)),
                obj,
                device=device,
            ),
            keep=args.num_rolling_checkpoints,
        )

    saver_best_cer = ckpt_saver("experiment.ckpt.lowest-valid-cer", experiment)
    saver_best_wer = ckpt_saver("experiment.ckpt.lowest-valid-wer", experiment)

    @action
    def save(saver, epoch):
        saver.save(suffix=epoch)

    # Set hooks
    trainer.add_hook(
        EPOCH_END,
        HookCollection(
            # Save on best CER
            Hook(Lowest(experiment.valid_cer()), Action(save, saver=saver_best_cer)),
            # Save on best WER
            Hook(Lowest(experiment.valid_wer()), Action(save, saver=saver_best_wer)),
        ),
    )
    if args.save_checkpoint_interval:
        # Save every `save_checkpoint_interval` epochs
        log.get_logger("laia.hooks.conditions.multiple_of").setLevel(log.WARNING)
        trainer.add_hook(
            EPOCH_END,
            Hook(
                MultipleOf(trainer.epochs, args.save_checkpoint_interval),
                Action(save, saver=ckpt_saver("experiment.ckpt", experiment)),
            ),
        )
    if args.max_nondecreasing_epochs:
        # Stop when the validation CER hasn't improved in
        # `max_nondecreasing_epochs` consecutive epochs
        trainer.add_hook(
            EPOCH_END,
            Hook(
                ConsecutiveNonDecreasing(
                    experiment.valid_cer(), args.max_nondecreasing_epochs
                ),
                trainer.stop,
            ),
        )
    if args.max_epochs:
        # Stop when `max_epochs` has been reached
        trainer.add_hook(
            EPOCH_START, Hook(GEqThan(trainer.epochs, args.max_epochs), trainer.stop)
        )

    # Continue from the given checkpoint, if possible
    StateCheckpointLoader(experiment, device=device).load_by(
        os.path.join(args.train_path, "experiment.{}".format(args.checkpoint))
    )

    experiment.run()

    # Experiment finished. Save the model separately
    ModelCheckpointSaver(
        CheckpointSaver(os.path.join(args.train_path, "model.ckpt")), model
    ).save(suffix="last")

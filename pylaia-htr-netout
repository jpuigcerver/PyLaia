#!/usr/bin/env python3

import argparse
import os
from collections import OrderedDict

import pytorch_lightning as pl
import torch

import laia.common.logging as log
from laia import get_installed_versions
from laia.callbacks import Netout, ProgressBar
from laia.common.arguments import (
    add_argument,
    add_defaults,
    add_lightning_args,
    args,
    group_to_namespace,
)
from laia.common.loader import ModelLoader, choose_by
from laia.engine import DataModule, EvaluatorModule
from laia.engine.feeders import Compose, ImageFeeder, ItemFeeder
from laia.losses.ctc_loss import transform_output
from laia.utils.kaldi import ArchiveLatticeWriter, ArchiveMatrixWriter


def main(args):
    log.info(f"Installed: {get_installed_versions()}")

    checkpoint_path = os.path.join(
        args.train_path, args.experiment_dirname, args.checkpoint
    )
    checkpoint = choose_by(checkpoint_path)
    if not checkpoint:
        log.error(f'Could not find the checkpoint="{checkpoint_path}"')
        exit(1)
    log.info(f'Using checkpoint="{checkpoint}')

    model = ModelLoader(args.train_path, filename=args.model_filename).load()
    if model is None:
        log.error('Could not find the model. Have you run "pylaia-htr-create-model"?')
        exit(1)
    state_dict = torch.load(checkpoint)["state_dict"]
    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k[len("model.") :]
        new_state_dict[name] = v
    model.load_state_dict(new_state_dict)

    evaluator_module = EvaluatorModule(
        model,
        batch_input_fn=Compose([ItemFeeder("img"), ImageFeeder()]),
        batch_id_fn=ItemFeeder("id"),
    )

    data_module = DataModule(
        img_dirs=args.img_dirs,
        color_mode=args.color_mode,
        batch_size=args.batch_size,
        te_img_list=args.img_list,
        stage="test",
    )

    writers = []
    if args.matrix is not None:
        writers.append(ArchiveMatrixWriter(args.matrix))
    if args.lattice is not None:
        writers.append(
            ArchiveLatticeWriter(args.lattice, digits=args.digits, negate=True)
        )
    if not writers:
        log.error("You did not specify any output file! Use --matrix and/or --lattice")
        exit(1)

    callbacks = [
        Netout(args.output_transform, writers, model_output_fn=transform_output),
        ProgressBar(refresh_rate=args.lightning.progress_bar_refresh_rate),
    ]

    trainer = pl.Trainer(
        default_root_dir=args.train_path,
        callbacks=callbacks,
        **vars(args.lightning),
    )
    trainer.test(evaluator_module, datamodule=data_module, verbose=False)


if __name__ == "__main__":
    parser = add_defaults(
        "batch_size",
        "train_path",
        "model_filename",
        "experiment_dirname",
        "color_mode",
        logging_level="WARNING",
    )
    add_argument(
        "img_list",
        type=argparse.FileType("r"),
        help="File containing images to decode. Doesn't require the extension",
    )
    add_argument(
        "checkpoint",
        type=str,
        help="Name of the model checkpoint to use, can be a glob pattern",
    )
    add_argument(
        "img_dirs",
        type=str,
        nargs="*",
        help=(
            "Directory containing word images. "
            "Optional if img_list contains whole paths"
        ),
    )
    add_argument(
        "--output_transform",
        type=str,
        default=None,
        choices=["softmax", "log_softmax"],
        help=(
            "Apply this transformation at the end of the model. "
            'For instance, use "softmax" to get posterior probabilities '
            "as the output of the model"
        ),
    )
    add_argument(
        "--matrix",
        type=argparse.FileType("wb"),
        default=None,
        help=(
            "Path of the Kaldi's archive containing the output matrices "
            "(one for each sample), where each row represents a timestep and "
            "each column represents a CTC label"
        ),
    )
    add_argument(
        "--lattice",
        type=argparse.FileType("w"),
        default=None,
        help=(
            "Path of the Kaldi's archive containing the output lattices"
            "(one for each sample), representing the CTC output"
        ),
    )
    add_argument(
        "--digits",
        type=int,
        default=10,
        help="Number of digits to be used for formatting",
    )

    # Add lightning default arguments to a group
    pl_group = parser.add_argument_group(title="pytorch-lightning arguments")
    pl_group = add_lightning_args(pl_group)

    args = args(parser=parser)

    # Move lightning default arguments to their own namespace
    args = group_to_namespace(args, pl_group, "lightning")
    # Delete some which will be set manually
    for a in ("default_root_dir",):
        delattr(args.lightning, a)

    main(args)
